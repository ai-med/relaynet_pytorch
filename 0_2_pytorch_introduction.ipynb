{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Introduction\n",
    "================\n",
    " \n",
    "<div class=\"alert alert-info\">\n",
    "    <strong>Note:</strong> This exercise is optional and only serves as an introduction and cheatsheet to the general concepts of PyTorch.\n",
    "</div>\n",
    "\n",
    "PyTorch is a scientific computing package for Python:\n",
    "\n",
    "-  Tensor and Neural Network computations (inparticular deep learning)\n",
    "-  Research oriented (in comparison to e.g. TensorFlow)\n",
    "-  Dynamic computational graph (in comparison to e.g. TensorFlow)\n",
    "-  “NumPy on the GPU”\n",
    "-  Backend and API heavily inspired by the original Torch written in Lua\n",
    "\n",
    "An in-depth tutorial of the concepts described in this notebook can be found [here](https://github.com/jcjohnson/pytorch-examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors\n",
    "=====\n",
    "\n",
    "The PyTorch `Tensor` class is very similar to the NumPy `ndarray` class. Their main distinction is the ability of PyTorch Tensors to be used on a GPU which lets them benefit from vastly accelerated and parallelized computations. In order to work with PyTorch it is crucial to understand the basic behavior of its `Tensor` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the initialization of a regular `5x3` matrix `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00 -3.6893e+19  0.0000e+00\n",
      "-3.6893e+19  1.1210e-44  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00 -3.6893e+19  0.0000e+00\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same matrix can be initialized with random entries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.3230  0.0162  0.5381\n",
      " 0.4730  0.2085  0.0520\n",
      " 0.5497  0.8563  0.8331\n",
      " 0.9552  0.2026  0.4311\n",
      " 0.4769  0.7794  0.1943\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of a `Tensor` can be retrieved with:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>`torch.Size` is in fact a tuple, so it supports the same operations as a regular Python tuple.</p>\n",
    "    <p>In contrast to a static computational graph of for example Tensorflow the dynamic graph of PyTorch allows to retrieve information such as its size at any time during runtime.</p>\n",
    "</div>\n",
    "\n",
    "Tensor Operations\n",
    "--------\n",
    "\n",
    "There are multiple syntaxes for `Tensor` operations. We illustrate the different options on the example of `Tensor` addition.\n",
    "\n",
    "Regular (NumPy) syntax:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.2997  0.7012  1.1823\n",
      " 0.4891  0.4527  0.3912\n",
      " 1.2358  1.6825  1.3753\n",
      " 1.0937  0.3184  0.5080\n",
      " 1.2402  1.0562  1.0772\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch syntax:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.2997  0.7012  1.1823\n",
      " 0.4891  0.4527  0.3912\n",
      " 1.2358  1.6825  1.3753\n",
      " 1.0937  0.3184  0.5080\n",
      " 1.2402  1.0562  1.0772\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch syntax with specific output variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.2997  0.7012  1.1823\n",
      " 0.4891  0.4527  0.3912\n",
      " 1.2358  1.6825  1.3753\n",
      " 1.0937  0.3184  0.5080\n",
      " 1.2402  1.0562  1.0772\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch syntax for inplace operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.2997  0.7012  1.1823\n",
      " 0.4891  0.4527  0.3912\n",
      " 1.2358  1.6825  1.3753\n",
      " 1.0937  0.3184  0.5080\n",
      " 1.2402  1.0562  1.0772\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>Any operation that mutates a `Tensor` in-place is post-fixed with an ``_``.</p>\n",
    "    <p>For example: ``x.copy_(y)``, ``x.t_()``, will copy  ``y`` to ``x``.</p>\n",
    "</div>\n",
    "\n",
    "`Tensor` indexing works just like standard NumPy indexing. And since recently PyTorch even supports `Tensor` [broadcasting](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html)!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0162\n",
      " 0.2085\n",
      " 0.8563\n",
      " 0.2026\n",
      " 0.7794\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy: There and back again\n",
    "---------------------------\n",
    "\n",
    "Converting a PyTorch `Tensor` to a NumPy `ndarray` and vice versa is a very simple. The `Tensor` and the `ndarray` will share the location of the underlying memory, and changing one will also change the other.\n",
    "\n",
    "Converting a `Tensor` to a `ndarray` works by simply calling the `Tensor.numpy()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the `Tensor` effects the `ndarray` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion from a `ndarray` to a `Tensor` is just as simple and holds the same properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `Tensor` allocated on the CPU (except the `torch.CharTensor`) support converting to\n",
    "NumPy and back.\n",
    "\n",
    "Tensors on the GPU\n",
    "------------------\n",
    "\n",
    "PyTorch Tensors can be moved onto a GPU using the ``Tensor.cuda()`` method. Before converting a GPU `Tensor` to NumPy it has to be moved back to the CPU by calling the ``Tensor.cpu()`` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available.\n"
     ]
    }
   ],
   "source": [
    "# first check if cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = x + y\n",
    "    \n",
    "    print(z)\n",
    "    print(z.cpu().numpy())\n",
    "else:\n",
    "    print(\"CUDA not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on PyTorch Tensors\n",
    "-----------------------\n",
    "\n",
    "The documentation of many more `Tensor` operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers can be found [here](http://pytorch.org/docs/torch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Autograd - automatic differentiation\n",
    "===================================\n",
    "\n",
    "Central to all neural networks in PyTorch is the ``autograd`` package. The package provides automatic differentiation for all operations on Tensors. PyTorch is a define-by-run framework, which means that the calculation of gradients ( e.g. during backpropagation) is defined at runtime and can be different at every single iteration.\n",
    "\n",
    "Variable\n",
    "--------\n",
    "\n",
    "``autograd.Variable`` is the central class of the package. It wraps the `Tensor` class, and supports nearly all of its operations. Once a computation graph is executed the ``Variable.backward()`` method can be used to automatically compute all the gradients.\n",
    "\n",
    "If ``Variable`` is not a scalar, the ``backward()`` method requires an additional ``grad_output`` argument which matches the shape of the ``Variable``. ``grad_output`` is supposed to be the gradient w.r.t the given output. For a scalar ``Variable`` ``grad_output`` is assumed to be `torch.Tensor([1.0])`.\n",
    "\n",
    "The underlying, raw tensor can be accessed with the ``Variable.data`` attribute and the\n",
    "gradient w.r.t. this variable is accumulated into ``Variable.grad``.\n",
    "\n",
    "<img src=\"http://pytorch.org/tutorials/_images/Variable.png\">\n",
    "\n",
    "In addition to the ``Variable`` class the `autograd` packages provides a `Function` class. The two classes are interconnected and build up an acyclic graph which encodes a complete history of computation. Each `Variable` has\n",
    "a ``Variable.grad_fn`` attribute which references the ``Function`` (e.g. an operation such as addition) that created the respective ``Variable`` and thereby determines its gradient. For Variables that were created by the user and not as a result of an operation the ``grad_fn`` attribute is ``None``.\n",
    "\n",
    "The following simple examples will illustrate the basic concepts of the ``autograd`` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Variable` object:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply an operation to the `Variable`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ``y`` was created as a result of an operation it has a ``grad_fn`` attribute (`Function`) unequal to `None`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x113471c18>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying more operations to `y` increases the computational graph:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients\n",
    "---------\n",
    "The gradient w.r.t the input `x` can now be computed (backpropagated) with ``out.backward()``. Remember for a scalar this is equivalent to doing ``out.backward(torch.Tensor([1.0]))``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input `x` was a `2x2` `Variable` and therefore $\\frac{d(out)}{dx}$ yields a matrix with the same shape:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a small computation graph the solution can easily be verified:\n",
    "\n",
    "The output w.r.t. the input is given as \n",
    "$$\n",
    "\\begin{align}\n",
    "    out =& \\frac{1}{4}\\sum_i z_i \\\\\n",
    "        =& \\frac{1}{4}\\sum_i 3y_i y_i \\\\\n",
    "        =& \\frac{1}{4}\\sum_i 3(x_i+2)^2\n",
    "\\end{align}\n",
    "$$.\n",
    "\n",
    "Therefore the gradient is $\\frac{\\partial out}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, which yields\n",
    "$\\frac{\\partial out}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$ for a particular input $x_i=1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `autograd` package in combination with the dynamic graph structure allow to do crazy things such as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1354.7946\n",
      "  333.7003\n",
      "    7.1815\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "\n",
    "The `Tensor` and `Variable` classes in combination with the `autograd` package build the foundation for constructing Neural Networks (NNs) with PyTorch. To further fascilitate the construction and training of a NN the ``torch.nn`` package, which depends on `autograd` to define NN models and differentiate them, includes additional NN-specifc classes and helper functions.\n",
    "\n",
    "For example the ``nn.Module`` class which works as a boilerplate NN model class and eventually contains all the individual layers and the ``Module.forward(x)`` method that infers the input ``x`` and returns the output of a NN.\n",
    "\n",
    "The following is a grapical illustration of the infamous *LeNet* NN from Yann LeCun. This NN was trained to classify the MNIST dataset of handwritten digit images:\n",
    "\n",
    "<img src=\"http://pytorch.org/tutorials/_images/mnist.png\">\n",
    "\n",
    "It is a simple feed-forward network which takes the input, feeds it through several layers one after the other, and then finally produces the classification output.\n",
    "\n",
    "\n",
    "Define *LeNet* with PyTorch\n",
    "--------------------------\n",
    "\n",
    "The following is an example implementation of the classification network above: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d (1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d (6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120)\n",
      "  (fc2): Linear(in_features=120, out_features=84)\n",
      "  (fc3): Linear(in_features=84, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Class constructor which preinitializes NN layers with trainable\n",
    "        parameters.\n",
    "        \"\"\"\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forwards the input x through each of the NN layers and outputs the result.\n",
    "        \"\"\"\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # An efficient transition from spatial conv layers to flat 1D fully \n",
    "        # connected layers is achieved by only changing the \"view\" on the\n",
    "        # underlying data and memory structure.\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        \"\"\"\n",
    "        Computes the number of features if the spatial input x is transformed\n",
    "        to a 1D flat input.\n",
    "        \"\"\"\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the `autograd` package a NN merely requires the definition of the ``Module.forward()`` method. The ``.backward()`` function (which backpropagtes the gradients) is automatically defined. Any `Tensor` operation is allowed in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``Module.parameters()``:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the `forward` method is an ``autograd.Variable``, and so is the output:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0139  0.1280 -0.0845  0.1228  0.0351  0.0402  0.0576  0.1114  0.1717  0.0598\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(1, 1, 32, 32))\n",
    "output = net(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before backpropagating for example a random gradient, the gradient buffers of all parameters should be set to zero:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "output.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>Calling the ``Variable.backward()`` method a second time before new inputs are forwarded will through an error. This is due to PyTorch deleting all the intermediary results in order to reduce memory consumption. Calling the ``.backward()`` method with the `retain_graph=True` argument keeps those results.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>The entire ``torch.nn`` package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "    For example, ``nn.Conv2d`` will take in a 4D Tensor of ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "    If you have a single sample, just use ``x.unsqueeze(0)`` to add a fake batch dimension.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "A loss function takes the (output, target) pair as inputs, and computes a value that estimates \"how far away\" the output is from the target.\n",
    "\n",
    "There are several different loss functions predefined under the `torch.nn` package. An example of a simple loss is the ``nn.MSELoss`` which computes the mean-squared error between the input and the target value.\n",
    "\n",
    "More examples of predefined losses are documented [here](http://pytorch.org/docs/nn.html#loss-functions).\n",
    "\n",
    "A MSE loss example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 37.6366\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(x)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target with 10 classes\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When ``loss.backward()`` is called, the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their ``Variable.grad`` attribute accumulated with the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagate the Loss\n",
    "--------------------\n",
    "\n",
    "A curical step for optimizing the network weights is the backpropogation of the loss. The nature of a computational graph makes this as easy as calling ``loss.backward()``. But since the gradients will be accumulated to already existing gradients one has to clear them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      "-0.0241\n",
      "-0.0340\n",
      " 0.0417\n",
      " 0.0052\n",
      "-0.1100\n",
      " 0.0043\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights Optimization\n",
    "------------------\n",
    "The simplest update rule used in practice for optimizing the weights of a NN is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Like any other NN component the optimization step can be implemented with the basic PyTorch classes.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_step(net):\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the PyTorch framework contains a small optimization package called ``torch.optim``. It includes various predefined update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>Common optimization options such as the L2-regularization (see ``weight_decay`` argument) are already included in the predefined optimization schemes.</p>\n",
    "</div>\n",
    "\n",
    "Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create an optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "\n",
    "# a single step of an example training loop\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(x)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update based on the accumalted gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap\n",
    "==============\n",
    "\n",
    "  -  ``torch.Tensor`` - A multi-dimensional array.\n",
    "  -  ``autograd.Variable`` - Wraps a `Tensor` and records the history of\n",
    "     operations applied to it. Has the same API as a ``Tensor``, with\n",
    "     some additions like ``backward()``. Also holds the gradient\n",
    "     w.r.t. the tensor.\n",
    "  -  ``nn.Module`` - Neural network module. Convenient way of\n",
    "     encapsulating parameters, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of `Variable`, that is automatically\n",
    "     registered as a parameter when assigned as an attribute to a\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements forward and backward definitions\n",
    "     of an autograd operation. Every ``Variable`` operation, creates at\n",
    "     least a single ``Function`` node, that connects to functions that\n",
    "     created a ``Variable`` and encodes its history.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>The `torchvision` package includes many predefined helper funcitons specifally designed for solving computer vision problems.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
